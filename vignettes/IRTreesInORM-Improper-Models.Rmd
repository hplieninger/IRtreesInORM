---
title: "Improper Models"
output: 
    prettydoc::html_pretty:
        theme: architect
        df_print: kable
bibliography: lib.bib
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Improper Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(digits = 2)

# path_results <- rprojroot::find_root_file(
#     "tools/sim-results.rda",
#     criterion = rprojroot::has_file("DESCRIPTION"))
# path_results <- rprojroot::find_package_root_file(
#     "tools/sim-results.rda")
```

```{r setup, include = FALSE}
library("conflicted")
library("dplyr")
conflict_prefer("filter", "dplyr")
library("purrr")
library("tidyr")
library("kableExtra")
```

This vignette contains a small simulation study, which was removed from the final version of the paper.
Herein, it is demonstrated that incorrect pseudo-items can lead to serious consequences and invalid conclusions.
The results match and explain published results [@lahuis_applying_2018; @lahuis_corrigendum_2019].

## Improper Models

The issue concerns parameter estimation on the basis of pseudo-items.
As described in the paper, an IR-tree model may be estimated in the form of a multidimensional, binary IRT model after the original items have been recoded into binary pseudo-items.
This might lead to the misconception that the pseudo-items can be defined independent of each other.
For example, a researcher may want to define the third pseudo-item $x^*_m$ as (0, 0, 1, 0, 0) instead of (-, 0, 1, 0, -) in the MCN model in order to reduce the number of missings.
Or, one might want to define the second pseudo-item $x^*_e$ as (1, 0, -, 0, 1) instead of (1, 0, 0, 0, 1) for some reason.
However, this is not legitimate within the IR-tree framework: This leads to a model (as implied by the pseudo-items) that no longer corresponds to the specified model equations and tree diagram.
Worse still, such changes can easily lead to an improper model in the sense that the model equations do not sum to 1 and that no tree diagram exists that could illustrate its structure.
Figuratively speaking, such an improper model may make predictions such as 50 \% heads and 60 \% tails.
Strictly speaking, the equations implied by the pseudo-items do not even comprise a probabilistic model, because the definition of a discrete probability distribution requires that the sum of the individual probabilities is 1.

Note that the estimation of an improper model may nevertheless converge without problems, because the software "doesn't know" that an IR-tree model for polytomous data is specified rather than a multidimensional IRT model with three seemingly independent, binary items.

Using an incorrect pseudo-item (i.e., model) will affect the empirical results.
However, it is not directly clear which outcomes will be affected and to what extent.
Therefore, a small simulation study was conducted.

## Simulation Study

The aim of the simulation was very specific, namely, to investigate the effects of a misspecified pseudo-item under known conditions.
Generating a very large sample allowed to analyze the data almost as if it were population data without much uncertainty due to sampling error.
Furthermore, the data-generating model was kept as simple as possible (e.g., by using only one instead of multiple target traits) in order to isolate the most important aspects.

### Method

A data set comprised of 10,000 persons and 27 items was generated from a population where the MCN model was the true model.
Therein, the variances of the latent dimensions were set to $\sigma^2_{\theta_t}=1.00$, $\sigma^2_{\theta_e}=0.75$, and $\sigma^2_{\theta_m}=0.50$.
Furthermore, the correlation of MRS $\theta_m$ and ERS $\theta_e$ was set to $r_{em}=-.50$, and the two remaining correlations were set to $r_{tm}=-.20$ and $r_{te}=.20$.
With respect to the item parameters, a low, an intermediate, and a high value was chosen for each of $\beta_m$, $\beta_e$, and $\beta_t$.
These were then fully crossed leading to $3^3=27$ combinations, one for each item.
The chosen values match estimates typically found in the IR-tree literature [e.g., @boeckenholt_modeling_2012; @plieninger_validity_2014], and the code, the simulated data, and further details can be found in the `code/` folder of this repository.

Three IR-tree models were then fit to the generated data.
First, the MCN model should lead to the correct estimates and best model fit, since it is the true, data-generating model.
Second, the ECN model was used, which should lead to inferior fit and estimates, since it is not the correct model in this case.
Third, the correct pseudo-item $x^*_e$ (1, 0, 0, 0, 1) of the MCN model was replaced with an incorrect pseudo-item (1, 0, -, 0, 1).
This model will be called *MCN2*, and it is an improper model in the sense described above.
This MCN2 model was used by @lahuis_applying_2018, and the present simulation study will allow to examine the artifacts introduced by the incorrect pseudo-item.
The three models were fit using Mplus 7.4 [@muthen_mplus_74].

### Results

```{r data}
# Import Mplus results

data("sim_res", package = "IRtreesInORM")

# Load true, data-generating parameters

data("sim_data", package = "IRtreesInORM")
thetas_true <- as.data.frame(sim_data$theta)
rm(sim_data)
```

#### Model Fit

As shown in the table below, the MCN model fit the data better than the ECN model according to AIC and BIC, and this was expected given that it was the true model.
Importantly, the MCN2 model showed much, much smaller values of AIC and BIC than the MCN model.
This is an artifact, because the MCN2 model is not a proper model, and it could not recover the true parameters as good as the fit indices spuriously imply.^[The (log-) likelihood of an IR-tree model is based on Equation (5) and (6), and thus contains products (sums) of the model parameters $\xi$ or $1-\xi$.
However, in the MCN2 model, one such term is removed from the likelihood of the MCN model, namely the term $1-e_{ij}$ for Category 3 (see Equation 9).
Thus, the deviance of the MCN2 model is necessarily smaller compared to the MCN model artificially implying better fit.
The amount of bias is a function of the number of responses in Category 3 and of the size of $e_{ij}$, and bias would be 0 in the unlikely case of no midpoint responses and/or all $e_{ij}=0$.]
Thus, it is expected that the pseudo-items used by @lahuis_applying_2018 artificially increased the fit of the MCN2 model in their data.

```{r}
sim_res %>%
    mutate(AIC = map_dbl(output, list("summaries", "AIC")),
           BIC = map_dbl(output, list("summaries", "BIC")),
           LL  = map_dbl(output, list("summaries", "LL")),
           par = map_dbl(output, list("summaries", "Parameters"))) %>% 
    mutate_at(vars(AIC:LL), round, -2) %>% 
    select(-output)
```

#### Recovery of Person Parameters Theta

The correlation of true and estimated parameters (ideally equal to 1) and the absolute bias $\lvert \theta - \hat{\theta} \rvert$ (ideally equal to 0) is shown in the following table for the person parameters $\theta$.
The results for the item parameters $\beta$ showed the same pattern but are not reported for the sake of brevity.
As expected, the recovery of the true parameters was best for the data-generating model, namely, the MCN model.
The proper but misspecified ECN model led to poorer recovery compared to the correct MCN model, which was expected.
Likewise, the improper MCN2 model also led to inferior recovery, especially for $\theta_e$ (which is explained by the fact that the corresponding pseudo-item $x^*_e$ is incorrectly defined in the MCN2 model).
In more detail, the absolute difference between the true and the estimated $\theta_e$ parameters was on average 0.12 larger in the MCN2 model compared to the MCN model, which corresponds to almost 1/7 of a standard deviation.
Importantly, this bias was dependent on the number of midpoint responses: For example, absolute bias was 0.30 for persons with at most 10 midpoint responses, but was 0.44 for persons with more than 10 midpoint responses.

```{r thetas, cache = TRUE}
# Extract estimated thetas

thetas <- sim_res %>%
    mutate(est = map(output, "savedata"),
           theta = map(est, ~select(.x, matches("theta\\d$"))))

# Add true thetas

thetas <- mutate(thetas, true = list(thetas_true))

# Comparison of estimated and true thetas across models

thetas_res <- thetas %>%
    mutate(corT   = map2_dbl(theta, true, ~cor(.x$THETA1, .y$V3)),
           corE   = map2_dbl(theta, true, ~cor(.x$THETA2, .y$V2)),
           corM   = map2_dbl(theta, true, ~cor(.x$THETA3, .y$V1)),
           # biasT  = map2_dbl(theta, true, ~mean(.y$V3 - .x$THETA1)),
           # biasE  = map2_dbl(theta, true, ~mean(.y$V2 - .x$THETA2)),
           # biasM  = map2_dbl(theta, true, ~mean(.y$V1 - .x$THETA3)),
           abiasT = map2_dbl(theta, true, ~mean(abs(.y$V3 - .x$THETA1))),
           abiasE = map2_dbl(theta, true, ~mean(abs(.y$V2 - .x$THETA2))),
           abiasM = map2_dbl(theta, true, ~mean(abs(.y$V1 - .x$THETA3)))) %>%
    select(Model, starts_with("cor"), starts_with("abias"))

kable(thetas_res, digits = 2) %>% 
    kable_styling() %>% 
    add_footnote(paste("Note: cor = Correlation of true and estimated theta;",
                   "abias = Absolute bias;",
                   "T = Trait; E = ERS; M = MRS."), notation = "none")

```

#### Recovery of Latent Variances and Correlations

A comparison of true and estimated latent variances and correlations showed that the estimates from the MCN model closely resembled the true values.
In contrast, the estimates from the MCN2 model differed substantially.
For example, the true correlation between the ERS and MRS dimension was set to -.50, it was correctly estimated in the MCN model, and it equaled -.06 in the MCN2 model, which would lead to dramatically different conclusions.

The true and estimated latent variances are depicted in the following table.

```{r}
sigma_res <- sim_res %>%
    mutate(est  = map(output, list("parameters", "unstandardized")),
           vars = map(est, ~filter(.x, paramHeader == "Variances")),
           vars = map(vars, ~select(.x, Var = est))) %>%
    mutate(std  = map(output, list("parameters", "stdyx.standardized")),
           cors = map(std, ~filter(.x, grepl("WITH", paramHeader))),
           cors = map(cors, ~select(.x, Cor = est)))

sigma_res %>% 
    select(Model, vars) %>% 
    unnest(cols = vars) %>% 
    mutate(Dim  = rep(c("T", "E", "M"), 3),
           True = rep(c(1, .75, .5), 3)) %>% 
    pivot_wider(names_from = Model, values_from = Var)
```

Furthermore, the true and estimated latent correlations are depicted in the following table.

```{r}
sigma_res %>% 
    select(Model, cors) %>% 
    unnest(cols = cors) %>% 
    mutate(Dims = rep(c("T,E", "T,M", "E,M"), 3),
           True = rep(c(.2, -.2, -.5), 3)) %>% 
    pivot_wider(names_from = Model, values_from = Cor)
```

## Discussion

In summary, the results of the present simulation study showed that parameter estimates are affected when using incorrect pseudo-items.
These effects are most severe for parameters that directly depend on an incorrect pseudo-item, but related parameters may be affected as well.
More importantly, model fit can also be disturbed, which may lead to wrong conclusions.
These results of the simulation study match and explain related empirical results [@lahuis_applying_2018; @lahuis_corrigendum_2019].

## References
